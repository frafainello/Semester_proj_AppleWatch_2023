{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"kbZ5UeQ1oxNV","executionInfo":{"status":"ok","timestamp":1705321943369,"user_tz":-60,"elapsed":21349,"user":{"displayName":"Francesco Fainello","userId":"08021983186610603633"}},"outputId":"94542b15-7d73-4a39-9acf-e578294d6876","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"XthZmNPPgrKo","executionInfo":{"status":"ok","timestamp":1705321943370,"user_tz":-60,"elapsed":5,"user":{"displayName":"Francesco Fainello","userId":"08021983186610603633"}}},"outputs":[],"source":["BASE_FOLDER = '/content/drive/MyDrive/SemesterProject/ML_modeling/'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"laO5XZTAgtdU","executionInfo":{"status":"ok","timestamp":1705321943370,"user_tz":-60,"elapsed":4,"user":{"displayName":"Francesco Fainello","userId":"08021983186610603633"}},"outputId":"987aa3fd-0b2b-4301-ba88-4c99ea0cd628","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/SemesterProject/ML_modeling\n"]}],"source":["cd '/content/drive/MyDrive/SemesterProject/ML_modeling/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GnQAOGouUBw"},"outputs":[],"source":["!pip install pytorch-lightning==1.9.4\n","!pip install torchtext\n","# !pip install pandas==1.3.5\n","!pip install pandas\n","!pip install torchsampler\n","!pip install wandb"]},{"cell_type":"markdown","metadata":{"id":"7uZN92ZqX6-1"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VcOfhRPxj2R"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPD67DnXqWsX"},"outputs":[],"source":["import numpy as np\n","import pickle\n","import torch\n","import pandas as pd\n","import seaborn as sns\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchmetrics.classification import Accuracy, F1Score, ConfusionMatrix\n","from multiprocessing import cpu_count\n","from torchsampler import ImbalancedDatasetSampler\n","\n","import os\n","from glob import glob\n","from multiprocessing import cpu_count\n","\n","from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n","from pytorch_lightning.loggers import TensorBoardLogger\n","\n","import wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBImPL3URg1u"},"outputs":[],"source":["from datasets import *\n","from models import *\n","from scheduler import *\n","from attention_maps import *\n","from helpers import *"]},{"cell_type":"markdown","metadata":{"id":"eEZzHTxtX-E3"},"source":["# Load Data"]},{"cell_type":"markdown","source":["Select wheter to use old or new data, or both. Default is training with both old and new, testing with new only"],"metadata":{"id":"fMrMKSBiAqmy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUPc5cOOhdsP"},"outputs":[],"source":["# TRAINING SET\n","train_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/old_first/train\",\n","              BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/train_new\"]    # Old and new\n","# train_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/train_new\"]  # New only\n","\n","# TESTING SET\n","# test_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/old_first/val\",\n","#              BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new\"]    # Old and new\n","test_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new\"]      # New only"]},{"cell_type":"markdown","source":["Label encoder"],"metadata":{"id":"HL2OOGxVAyJx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xgzkvcoPs3OP"},"outputs":[],"source":["class LabelEncoder():\n","    def __init__(self):\n","        self.encode_map = {\n","            'Eating': 0,\n","            'Face touching': 0,\n","            'Eye rubbing': 1,\n","            'Eye rubbing light': 1,\n","            'Eye rubbing moderate': 1,\n","            'Eye touching': 0,\n","            'Glasses readjusting': 0,\n","            'Hair combing': 2,\n","            'Make up': 0,\n","            'Make up application': 0,\n","            'Make up removal': 0,\n","            'Skin scratching': 2,\n","            'Teeth brushing': 3,\n","            'Nothing': 4,\n","            'no_label': 4\n","        }\n","        self.decode_map = {\n","            0: \"Face touching\",\n","            1: \"Eye rubbing\",\n","            2: \"Hair combing & Skin scratching\",\n","            # 3: \"Skin scratching\",\n","            3: \"Teeth brushing\",\n","            4: \"Nothing\"\n","        }\n","\n","    def transform(self, labels):\n","        return [self.encode_map[label] for label in labels]\n","\n","    def inv_transform(self, labels):\n","        return [self.decode_map[label] for label in labels]\n","\n","\n","\n","label_encoder = LabelEncoder()\n","CLASSES = list(label_encoder.decode_map.values())\n","\n","FEATURES = ['accelerometerAccelerationX(G)',\n","            'accelerometerAccelerationY(G)',\n","            'accelerometerAccelerationZ(G)',\n","            'motionYaw(rad)',\n","            'motionRoll(rad)',\n","            'motionPitch(rad)',\n","            'motionRotationRateX(rad/s)',\n","            'motionRotationRateY(rad/s)',\n","            'motionRotationRateZ(rad/s)',\n","            'motionUserAccelerationX(G)',\n","            'motionUserAccelerationY(G)',\n","            'motionUserAccelerationZ(G)',\n","            'motionQuaternionX(R)',\n","            'motionQuaternionY(R)',\n","            'motionQuaternionZ(R)',\n","            'motionQuaternionW(R)',\n","            'motionGravityX(G)',\n","            'motionGravityY(G)',\n","            'motionGravityZ(G)'\n","]"]},{"cell_type":"markdown","metadata":{"id":"kin2L6lvXhez"},"source":["# Data analysis"]},{"cell_type":"markdown","source":["Brief analysis of data distribution"],"metadata":{"id":"pEE6qtxdA5Qe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIggNKc3pgdh"},"outputs":[],"source":["label_encoder = LabelEncoder()\n","stats_train = pd.DataFrame(columns = ['0','1','2','3','4'])\n","stats_test = pd.DataFrame(columns = ['0','1','2','3','4'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdSuAhpbXlDh"},"outputs":[],"source":["train_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/old_first/train\",\n","              BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/train_new\"]\n","test_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new\"]\n","\n","for path in train_path:\n","  for subpath, subdir, files in os.walk(path):\n","      for file in glob(os.path.join(subpath, \"*.csv\")):\n","          df = pd.read_csv(file)\n","\n","          try:\n","            user = df['user'][0]\n","          except:\n","            user = df['userName'][0]\n","\n","          label = df['label'][0]\n","          label = str(label_encoder.encode_map[label])\n","\n","          if not (stats_train.index == user).any():\n","            stats_train.loc[user] = list(np.zeros(stats_train.shape[1]))\n","\n","          stats_train.loc[user][label] += 1\n","\n","for path in test_path:\n","  for subpath, subdir, files in os.walk(path):\n","      for file in glob(os.path.join(subpath, \"*.csv\")):\n","          df = pd.read_csv(file)\n","\n","          try:\n","            user = df['user'][0]\n","          except:\n","            user = df['userName'][0]\n","\n","          label = df['label'][0]\n","          label = str(label_encoder.encode_map[label])\n","\n","          if not (stats_test.index == user).any():\n","            stats_test.loc[user] = list(np.zeros(stats_test.shape[1]))\n","\n","          stats_test.loc[user][label] += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JlirM3JiKv8x"},"outputs":[],"source":["stats_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7DeDNLAsPG0"},"outputs":[],"source":["stats_test"]},{"cell_type":"markdown","metadata":{"id":"zB6mF5FaYEo8"},"source":["# Train Model"]},{"cell_type":"markdown","metadata":{"id":"UwQcam5qmIB4"},"source":["### Train baseline models"]},{"cell_type":"markdown","source":["Train DeepConvLSTM model"],"metadata":{"id":"4JSgKXLuBLY6"}},{"cell_type":"code","source":["# Set training parameters and load data\n","num_epochs = 30\n","bsz = 50\n","pl.seed_everything(42)\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, bsz, normalize = True, add_fft = False)"],"metadata":{"id":"SoJX4Q0jEaw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83YbY8QtuEAl"},"outputs":[],"source":["model = DeepConvLSTM(n_features=len(FEATURES), n_classes=len(CLASSES), learning_rate = 1e-4, warmup = 1, warmup_iters=1e7, datamodule = data_module)\n","\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath = BASE_FOLDER + \"logs\" + model.__class__.__name__,\n","    filename = \"checkpoint\",\n","    save_top_k = 1, verbose=True,\n","    monitor = \"f1\", mode=\"max\")\n","\n","lr_monitor = LearningRateMonitor(logging_interval='step')\n","logger = TensorBoardLogger(save_dir=BASE_FOLDER, name='logs' + model.__class__.__name__)\n","\n","trainer = pl.Trainer(\n","    max_epochs=num_epochs,\n","    devices=1,\n","    logger=logger,\n","    callbacks=[checkpoint_callback, lr_monitor],\n","    accelerator='gpu'\n",")\n","\n","trainer.fit(model, data_module)\n","\n","# Save model arguments\n","dir = BASE_FOLDER + 'logs' + model.__class__.__name__\n","dir += '/kwargs.pt'\n","dir = uniquify(dir)\n","torch.save((model.kwargs, model.state_dict(), model.fig, model.best_f1), dir)"]},{"cell_type":"markdown","source":["Load model and visualize results"],"metadata":{"id":"JnpHAa8sFoOY"}},{"cell_type":"code","source":["kwargs_dir = BASE_FOLDER + 'logsDeepConvLSTM/kwargs.pt'\n","check_dir = BASE_FOLDER + 'logsDeepConvLSTM/checkpoint.ckpt'\n","\n","kwargs, _ , confmat, f1= torch.load(kwargs_dir)\n","model = DeepConvLSTM.load_from_checkpoint(check_dir, **kwargs)"],"metadata":{"id":"GncFAwIbFoOY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f1)\n","confmat"],"metadata":{"id":"9mi62G8lFoOY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Validate for different test set if necessary"],"metadata":{"id":"tGN82_NeCIfs"}},{"cell_type":"code","source":["train_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/train_new\"]\n","test_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user52\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user54\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user57\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user58\"]\n","\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, 1, normalize = True, add_fft = False)\n","\n","trainer = pl.Trainer(\n","    max_epochs=10,\n","    devices=1,\n","    accelerator='gpu'\n",")\n","trainer.validate(model=model, ckpt_path=check_dir, verbose=True, datamodule=data_module)"],"metadata":{"id":"stztlBZGM-Zz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_dir = BASE_FOLDER + 'plots/confmat' + model.__class__.__name__ + '.png'\n","img_dir = uniquify(img_dir)\n","model.fig.savefig(img_dir)"],"metadata":{"id":"XzW6Ep28M9hJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3oGrHyzfDTO"},"source":["### Train attention model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKFUHUbkfDTV"},"outputs":[],"source":["# Set training parameters and load data\n","num_epochs = 30\n","bsz = 50\n","pl.seed_everything(42)\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, bsz, normalize = True, add_fft = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7-4o0aIfDTW"},"outputs":[],"source":["model = AttentionModel(n_features=len(FEATURES),\n","                       n_classes=len(CLASSES),\n","                       datamodule=data_module,\n","                       learning_rate = 1e-3,\n","                       seq_length = 150,\n","                       input_dim=19,\n","                       model_dim=128,\n","                       num_heads= 4,\n","                       num_layers=4,\n","                       warmup=100,\n","                       warmup_iters=10000,\n","                       dropout=0.1)\n","\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath = BASE_FOLDER + \"logs\" + model.__class__.__name__,\n","    filename = \"checkpoint\",\n","    save_top_k = 1, verbose=True,\n","    monitor = \"f1\", mode=\"max\")\n","\n","lr_monitor = LearningRateMonitor(logging_interval='step')\n","\n","logger = TensorBoardLogger(save_dir=BASE_FOLDER, name='logs' + model.__class__.__name__)\n","\n","trainer = pl.Trainer(\n","    max_epochs=num_epochs,\n","    devices=1,\n","    logger=logger,\n","    callbacks=[checkpoint_callback, lr_monitor],\n","    accelerator='gpu'\n",")\n","\n","trainer.fit(model, data_module)\n","\n","\n","# Save model arguments\n","dir = BASE_FOLDER + 'logs' + model.__class__.__name__\n","dir += '/kwargs.pt'\n","dir = uniquify(dir)\n","torch.save((model.kwargs, model.state_dict(), model.fig, model.best_f1), dir)"]},{"cell_type":"markdown","source":["Tensorboard logger"],"metadata":{"id":"PJcp1qr2CmuE"}},{"cell_type":"code","source":["!kill 4935\n","%reload_ext tensorboard\n","%tensorboard --logdir logsAttentionModel/version_0 --samples_per_plugin images=30"],"metadata":{"id":"CU9V-3h3fDTW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load model"],"metadata":{"id":"NtBPGs3EfDTW"}},{"cell_type":"code","source":["kwargs_dir = BASE_FOLDER + 'logsAttentionModel/kwargs.pt'\n","check_dir = BASE_FOLDER + 'logsAttentionModel/checkpoint.ckpt'\n","\n","kwargs, _ , confmat, f1= torch.load(kwargs_dir)\n","model = AttentionModel.load_from_checkpoint(check_dir, **kwargs)"],"metadata":{"id":"xi5LSNjNfDTW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f1)\n","confmat"],"metadata":{"id":"wiSHJyDjfDTX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_dir = BASE_FOLDER + 'plots/confmat' + model.__class__.__name__ + '.png'\n","img_dir = uniquify(img_dir)\n","confmat.savefig(img_dir)"],"metadata":{"id":"LWMcYXI0-NVY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Validate for different test set"],"metadata":{"id":"wO8jL8BnCbr8"}},{"cell_type":"code","source":["train_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/train_new\"]\n","test_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user52\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user54\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user57\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user58\"]\n","\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, 1, normalize = True, add_fft = False)\n","\n","trainer = pl.Trainer(\n","    max_epochs=10,\n","    devices=1,\n","    accelerator='gpu'\n",")\n","\n","trainer.validate(model=model, ckpt_path=check_dir, verbose=True, datamodule=data_module)"],"metadata":{"id":"wLF7uFaqfDTX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_dir = BASE_FOLDER + 'plots/confmat' + model.__class__.__name__ + '.png'\n","img_dir = uniquify(img_dir)\n","model.fig.savefig(img_dir)"],"metadata":{"id":"nIe9X5PoM2V6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attention maps"],"metadata":{"id":"Krqilus1fDTX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4U4ojHBfDTX"},"outputs":[],"source":["train_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/train_new\"]\n","test_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user52\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user54\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user57\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user58\"]\n","\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, 1, normalize = True, add_fft = False)\n","data_module.setup()"]},{"cell_type":"code","source":["it = iter(data_module.val_dataloader())\n","data_input, labels = next(it)\n","for l in list(label_encoder.decode_map.keys()):\n","    correct_found = False\n","    while not correct_found:\n","        data_input, labels = next(it)\n","        if labels == torch.tensor(l):\n","            y_hat = model.forward(data_input)\n","            y_pred = torch.argmax(y_hat, dim=1)\n","            if y_pred == l:\n","                correct_found = True\n","\n","    attention_maps = model.get_attention_maps(data_input)\n","\n","    label_str = label_encoder.decode_map[labels.item()]\n","    filename = BASE_FOLDER + 'plots/attention_maps/' + model.__class__.__name__ + '_' + label_str +'.png'\n","    plot_attention_maps(data_input, label_str , attention_maps, idx=0, filename=filename)\n","    it = iter(data_module.val_dataloader())"],"metadata":{"id":"pTMtShXOB183"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RpN0BkDMKv82"},"source":["### Train attention model with LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vR5V6RNC0tDs"},"outputs":[],"source":["# Set training parameters and load data\n","num_epochs = 40\n","bsz = 50\n","pl.seed_everything(42)\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, bsz, normalize = True, add_fft = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvQCmSRwKv82"},"outputs":[],"source":["model = LSTM_AttentionModel(n_features=len(FEATURES),\n","                       n_classes=len(CLASSES),\n","                       datamodule=data_module,\n","                       learning_rate = 1e-3,\n","                       seq_length = 150,\n","                       input_dim=19,\n","                       model_dim=128,\n","                       num_heads=4,\n","                       lstm_layers = 2,\n","                       num_layers=4,\n","                       warmup=100,\n","                       warmup_iters=10000,\n","                       dropout=0.1)\n","\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath = BASE_FOLDER + \"logs\" + model.__class__.__name__,\n","    filename = \"checkpoint\",\n","    save_top_k = 1, verbose=True,\n","    monitor = \"f1\", mode=\"max\")\n","\n","lr_monitor = LearningRateMonitor(logging_interval='step')\n","\n","logger = TensorBoardLogger(save_dir=BASE_FOLDER, name='logs' + model.__class__.__name__)\n","\n","trainer = pl.Trainer(\n","    max_epochs=num_epochs,\n","    devices=1,\n","    logger=logger,\n","    callbacks=[checkpoint_callback, lr_monitor],\n","    accelerator='gpu'\n",")\n","\n","trainer.fit(model, data_module)\n","\n","\n","# Save model arguments\n","dir = BASE_FOLDER + 'logs' + model.__class__.__name__\n","dir += '/kwargs.pt'\n","dir = uniquify(dir)\n","torch.save((model.kwargs, model.state_dict(), model.fig, model.best_f1), dir)"]},{"cell_type":"markdown","source":["Tensorboard logger"],"metadata":{"id":"aDhW2K5sCuyU"}},{"cell_type":"code","source":["!kill 83255\n","%reload_ext tensorboard\n","%tensorboard --logdir logsLSTM_AttentionModel/best_5lab --samples_per_plugin images=50"],"metadata":{"id":"SQZ0JBsZlBMg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load model"],"metadata":{"id":"fdqZaH2FsLkv"}},{"cell_type":"code","source":["kwargs_dir = BASE_FOLDER + 'logsLSTM_AttentionModel/best_5lab_kwargs.pt'\n","check_dir = BASE_FOLDER + 'logsLSTM_AttentionModel/best_5lab.ckpt'\n","\n","kwargs, _ , confmat, f1 = torch.load(kwargs_dir)\n","model = LSTM_AttentionModel.load_from_checkpoint(check_dir, **kwargs)"],"metadata":{"id":"KX_w77XBZFDG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f1)\n","confmat"],"metadata":{"id":"EzjHH2WVeu8p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Validate on different test set"],"metadata":{"id":"JDk2Hn7WDDTg"}},{"cell_type":"code","source":["train_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/train_new\"]\n","test_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user52\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user54\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user57\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user58\"]\n","\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, 1, normalize = True, add_fft = False)\n","\n","trainer = pl.Trainer(\n","    max_epochs=10,\n","    devices=1,\n","    accelerator='gpu'\n",")\n","trainer.validate(model=model, ckpt_path=check_dir, verbose=True, datamodule=data_module)"],"metadata":{"id":"e79ppazKKxRx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_dir = BASE_FOLDER + 'plots/confmat' + model.__class__.__name__ + '.png'\n","img_dir = uniquify(img_dir)\n","model.fig.savefig(img_dir)"],"metadata":{"id":"EFAiIzMtHeiI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.best_f1"],"metadata":{"id":"ipY4p7GvSpOK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Attention maps"],"metadata":{"id":"uACuyfXuZF_X"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Gw5tTH-6l45"},"outputs":[],"source":["train_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/train_new\"]\n","test_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user52\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user54\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user57\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user58\"]\n","\n","\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, 1, normalize = True, add_fft = False)\n","data_module.setup()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Go0fs79227Yq"},"outputs":[],"source":["it = iter(data_module.val_dataloader())\n","data_input, labels = next(it)\n","for l in list(label_encoder.decode_map.keys()):\n","    correct_found = False\n","    while not correct_found:\n","        data_input, labels = next(it)\n","        if labels == torch.tensor(l):\n","            y_hat = model.forward(data_input)\n","            y_pred = torch.argmax(y_hat, dim=1)\n","            if y_pred == l:\n","                correct_found = True\n","\n","    attention_maps = model.get_attention_maps(data_input)\n","\n","    label_str = label_encoder.decode_map[labels.item()]\n","    filename = BASE_FOLDER + 'plots/attention_maps/' + model.__class__.__name__ + '_' + label_str +'.png'\n","    plot_attention_maps(data_input, label_str , attention_maps, idx=0, filename=filename)\n","    it = iter(data_module.val_dataloader())"]},{"cell_type":"markdown","metadata":{"id":"dX_aoGs0-jKJ"},"source":["# Tune hyperparameters\n","Uses online resource WandB\n","Random search of the best hyperparameter set is performed by running training for permutations of preset parameters, and recording F1 scores. Possibility of visualizing plots and results on wandb sweep website, link provided by cells below"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYpte1Hx_cuG"},"outputs":[],"source":["wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"w62RtsalDTNQ"},"source":["Define the sweep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6EiuEol_7Cp"},"outputs":[],"source":["# Choose searching method\n","sweep_config = {\n","    'method' : 'random'\n","}\n","\n","# Define objective metric\n","metric = {\n","    'name' : 'f1',\n","    'goal' : 'maximize'\n","}\n","\n","sweep_config['metric'] = metric\n","\n","# Set values for hyperparameters\n","parameters_dict = {\n","    'n_layers': {\n","        'values' : [1,2,4]\n","    },\n","    'lstm_layers': {\n","        'values' : [1,2]\n","    },\n","    'n_heads': {\n","        'values' : [2,4]\n","    },\n","    'emb_dim': {\n","        'values' : [64,128,256]\n","    },\n","    'lr': {\n","        'values' : [1e-3]\n","    },\n","    'warmup': {\n","        'values' : [100,200]\n","    },\n","    'dropout': {\n","        'values' : [0.1,0.2]\n","    }\n","}\n","\n","sweep_config['parameters'] = parameters_dict\n","\n","# Directly set fixed parameters\n","# parameters_dict.update({\n","#     'seq_length' : {\n","#         'value': 50\n","#     }\n","# })\n","\n","# Visualize dictionary\n","# import pprint\n","# pprint.pprint(sweep_config)"]},{"cell_type":"markdown","metadata":{"id":"yETUDa2_DVlI"},"source":["Initialize the sweep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"033H2oCVDXuD"},"outputs":[],"source":["sweep_id = wandb.sweep(sweep_config, project = 'Semester')"]},{"cell_type":"markdown","metadata":{"id":"19xUInhRD1pM"},"source":["Run the sweep agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQBzVCYcD3AX"},"outputs":[],"source":["def tuning(config=None):\n","    # Initialize a new wandb run\n","    with wandb.init(config=config):\n","        # If called by wandb.agent, as below,\n","        # this config will be set by Sweep Controller\n","        config = wandb.config\n","\n","        num_epochs = 30\n","\n","        bsz = 50\n","        data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, bsz, normalize = True, add_fft = False)\n","\n","        # # WITHOUT LSTM\n","        # model = AttentionModel(n_features=len(FEATURES),\n","        #                        n_classes=len(CLASSES),\n","        #                        datamodule=data_module,\n","        #                        learning_rate = config.lr,\n","        #                        seq_length = 50,\n","        #                        input_dim=19,\n","        #                        model_dim=config.emb_dim*config.n_heads,\n","        #                        num_heads=config.n_heads,\n","        #                        num_layers=config.n_layers,\n","        #                        warmup=config.warmup,\n","        #                        dropout=config.dropout)\n","\n","        # WITH LSTM\n","        model = LSTM_AttentionModel(n_features=len(FEATURES),\n","                              n_classes=len(CLASSES),\n","                              datamodule=data_module,\n","                              learning_rate = config.lr,\n","                              seq_length = 150,\n","                              input_dim=19,\n","                              model_dim=config.emb_dim*config.n_heads,\n","                              num_heads=config.n_heads,\n","                              lstm_layers = config.lstm_layers,\n","                              num_layers=config.n_layers,\n","                              warmup=config.warmup,\n","                              dropout=config.dropout)\n","\n","        # WITH GPU\n","        trainer = pl.Trainer(\n","            max_epochs=num_epochs,\n","            devices=1,\n","            accelerator='gpu'\n","        )\n","\n","        trainer.fit(model, data_module)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KFcAAS6Fr_e"},"outputs":[],"source":["# Set training parameters and load data\n","pl.seed_everything(42)\n","\n","# UNCOMMENT WANDB LINES IN models.py\n","wandb.agent(sweep_id, tuning, count=50)"]},{"cell_type":"markdown","metadata":{"id":"kDSpr5w7A6D1"},"source":["# Transfer learning"]},{"cell_type":"code","source":["class LabelEncoder():\n","    def __init__(self):\n","        self.encode_map = {\n","            'Eating': 0,\n","            'Face touching': 0,\n","            'Eye rubbing': 1,\n","            'Eye rubbing light': 1,\n","            'Eye rubbing moderate': 1,\n","            'Eye touching': 0,\n","            'Glasses readjusting': 0,\n","            'Hair combing': 2,\n","            'Make up': 0,\n","            'Make up application': 0,\n","            'Make up removal': 0,\n","            'Skin scratching': 2,\n","            'Teeth brushing': 3,\n","            'Nothing': 4,\n","            'no_label': 4\n","        }\n","        self.decode_map = {\n","            0: \"Face touching\",\n","            1: \"Eye rubbing\",\n","            2: \"Hair combing & Skin scratching\",\n","            # 3: \"Skin scratching\",\n","            3: \"Teeth brushing\",\n","            4: \"Nothing\"\n","        }\n","\n","    def transform(self, labels):\n","        return [self.encode_map[label] for label in labels]\n","\n","    def inv_transform(self, labels):\n","        return [self.decode_map[label] for label in labels]\n","\n","\n","\n","label_encoder = LabelEncoder()\n","CLASSES = list(label_encoder.decode_map.values())\n","\n","FEATURES = ['accelerometerAccelerationX(G)',\n","            'accelerometerAccelerationY(G)',\n","            'accelerometerAccelerationZ(G)',\n","            'motionYaw(rad)',\n","            'motionRoll(rad)',\n","            'motionPitch(rad)',\n","            'motionRotationRateX(rad/s)',\n","            'motionRotationRateY(rad/s)',\n","            'motionRotationRateZ(rad/s)',\n","            'motionUserAccelerationX(G)',\n","            'motionUserAccelerationY(G)',\n","            'motionUserAccelerationZ(G)',\n","            'motionQuaternionX(R)',\n","            'motionQuaternionY(R)',\n","            'motionQuaternionZ(R)',\n","            'motionQuaternionW(R)',\n","            'motionGravityX(G)',\n","            'motionGravityY(G)',\n","            'motionGravityZ(G)'\n","]"],"metadata":{"id":"plfnB_hHDT6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aVr7XIih_CJb"},"source":["Split users originally for testing in training and testing. Ignore \"no label\" class (too few data points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rEA9CYhr4hcs"},"outputs":[],"source":["import shutil\n","\n","user_path = [BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user52\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user54\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user57\",\n","             BASE_FOLDER + \"HFI_data/HFI_data/HandFace_data/new/test_new/user58\",]\n","\n","DESTINATION = BASE_FOLDER + \"HFI_data/Transfer_data_withNO/\"\n","users = ['user52','user54','user57','user58']\n","\n","for ind, path in enumerate(user_path):\n","    user_name = users[ind]\n","    print(user_name+'...')\n","\n","    label_counter = np.zeros(len(label_encoder.decode_map)) # !!!!!\n","\n","    if not os.path.exists(DESTINATION + user_name + \"/train/\"):\n","        os.makedirs(os.path.dirname(DESTINATION + user_name + \"/train/\"), exist_ok=True)\n","        os.makedirs(os.path.dirname(DESTINATION + user_name + \"/test/\"), exist_ok=True)\n","\n","    for subpath, subdir, files in os.walk(path):\n","        for file in glob(os.path.join(subpath, \"*.csv\")):\n","            df = pd.read_csv(file)\n","\n","            label = df['label'][0]\n","            label = label_encoder.encode_map[label]\n","\n","            file_name = os.path.basename(os.path.normpath(file))\n","\n","            if label_counter[label] < 5 and label != len(label_encoder.decode_map)-1:\n","                label_counter[label] += 1\n","                shutil.copyfile(file, DESTINATION + user_name + \"/train/\" + file_name)\n","            else:\n","                shutil.copyfile(file, DESTINATION + user_name + \"/test/\" + file_name)\n","    try:\n","        print(df['user'][0])\n","    except:\n","        print(df['userName'][0])"]},{"cell_type":"markdown","metadata":{"id":"c94Suu2z_Mm3"},"source":["Load best model"]},{"cell_type":"code","source":["kwargs_dir = BASE_FOLDER + 'logsLSTM_AttentionModel/best_5lab_kwargs.pt'\n","check_dir = BASE_FOLDER + 'logsLSTM_AttentionModel/best_5lab.ckpt'\n","\n","kwargs, _ , confmat, f1= torch.load(kwargs_dir)\n","model = LSTM_AttentionModel.load_from_checkpoint(check_dir, **kwargs)"],"metadata":{"id":"xlJSzWfGfoVM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluate model on user test set"],"metadata":{"id":"z8aVl0__fp9C"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UsIHBXAwqdg7"},"outputs":[],"source":["user = 'user54'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAHLKlnoj8BD"},"outputs":[],"source":["# Load data\n","train_path = [BASE_FOLDER + \"HFI_data/Transfer_data_withNO/\" + user + \"/train\"]\n","test_path = [BASE_FOLDER + \"HFI_data/Transfer_data_withNO/\" + user + \"/test\"]\n","bsz = 1\n","pl.seed_everything(42)\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, bsz, normalize = True)"]},{"cell_type":"code","source":["trainer = pl.Trainer(\n","    max_epochs=10,\n","    devices=1,\n","    accelerator='gpu'\n",")\n","model.best_f1 = 0\n","trainer.validate(model=model, ckpt_path=check_dir, verbose=True, datamodule=data_module)"],"metadata":{"id":"t45gfYokgIHl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.best_f1)\n","model.fig"],"metadata":{"id":"UNGdpUwdgO6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_dir = BASE_FOLDER + 'plots/transfer_learning/confmat' + model.__class__.__name__ + user + '_before.png'\n","img_dir = uniquify(img_dir)\n","model.fig.savefig(img_dir)"],"metadata":{"id":"GlN1oZKJNNNa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cufaoXa1_QNo"},"source":["Train last layer for user (transfer learning)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMNo1C2ifSE4"},"outputs":[],"source":["# Load model, freeze layers\n","for ind, param in enumerate(model.parameters()):\n","    param.requires_grad = False\n","# Unfreeze last layer\n","for param in model.classifier.parameters():\n","    param.requires_grad = True\n","\n","# Change learning rate parameters\n","model.learning_rate = 1e-3\n","model.warmup = 1\n","model.warmup_iters = 1e6\n","\n","# Load data\n","bsz = 1\n","pl.seed_everything(42)\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, bsz, normalize = True)\n","model.datamodule=data_module\n","\n","# Train\n","pl.seed_everything(42)\n","num_epochs = 5\n","\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath = BASE_FOLDER + \"logs_transfer_\" + model.__class__.__name__+ '/'+user,\n","    filename = \"checkpoint\",\n","    save_top_k = 1, verbose=True,\n","    monitor = \"f1\", mode=\"max\")\n","\n","lr_monitor = LearningRateMonitor(logging_interval='step')\n","\n","logger = TensorBoardLogger(save_dir=BASE_FOLDER, name='logs_transfer_' + model.__class__.__name__+ '/'+user)\n","\n","trainer = pl.Trainer(\n","    max_epochs=num_epochs,\n","    devices=1,\n","    logger=logger,\n","    callbacks=[checkpoint_callback, lr_monitor],\n","    accelerator='gpu'\n",")\n","\n","trainer.fit(model, data_module)\n","\n","\n","# Save model arguments\n","dir = BASE_FOLDER + 'logs_transfer_' + model.__class__.__name__+ '/'+user\n","dir += '/kwargs.pt'\n","dir = uniquify(dir)\n","torch.save((model.kwargs, model.state_dict(), model.fig, model.best_f1), dir)"]},{"cell_type":"markdown","metadata":{"id":"zvjkOL_Z_VrY"},"source":["Evaluate results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbGm2Pe1NQQQ"},"outputs":[],"source":["# Load data\n","train_path = [BASE_FOLDER + \"HFI_data/Transfer_data_withNO/\" + user + \"/train\"]\n","test_path = [BASE_FOLDER + \"HFI_data/Transfer_data_withNO/\" + user + \"/test\"]\n","bsz = 1\n","pl.seed_everything(42)\n","data_module = HandFaceDataModule(train_path, test_path, FEATURES, label_encoder, bsz, normalize = True)"]},{"cell_type":"code","source":["check_dir = BASE_FOLDER + 'logs_transfer_LSTM_AttentionModel/'+user+'/5labwithNO.ckpt'\n","kwargs_dir = BASE_FOLDER + 'logs_transfer_LSTM_AttentionModel/'+user+'/5labwithNO_kwargs.pt'\n","\n","kwargs, _ , confmat, f1= torch.load(kwargs_dir)\n","model = LSTM_AttentionModel.load_from_checkpoint(check_dir, **kwargs)\n","\n","trainer = pl.Trainer(\n","    max_epochs=10,\n","    devices=1,\n","    accelerator='gpu'\n",")\n","model.best_f1 = 0\n","trainer.validate(model=model, ckpt_path=check_dir, verbose=True, datamodule=data_module)"],"metadata":{"id":"ncqiOUkNNQQX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.best_f1)\n","model.fig"],"metadata":{"id":"iI_NucVQgyEn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_dir = BASE_FOLDER + 'plots/transfer_learning/confmat' + model.__class__.__name__ + user + '_after.png'\n","img_dir = uniquify(img_dir)\n","model.fig.savefig(img_dir)"],"metadata":{"id":"r-M_WtYXNa-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jIs0ikVYP3yN"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":0}